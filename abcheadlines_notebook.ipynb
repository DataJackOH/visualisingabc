{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9cbc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.animation as ani\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from pandas.tseries.offsets import *\n",
    "import numpy as np\n",
    "import pandas_alive\n",
    "import math\n",
    "\n",
    "from plotly.subplots                       import make_subplots\n",
    "import plotly.express                      as ex\n",
    "import plotly.graph_objs                   as go\n",
    "import plotly.offline                      as pyo\n",
    "\n",
    "\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "from tqdm.notebook                         import tqdm\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "#%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bb0f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "##readabc\n",
    "df = pd.read_csv('/Users/jackohagan/datajournalism/abcheadlines/abcnews-date-text.csv',parse_dates=[0], infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4fa8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert date to start of month\n",
    "df['publish_date'] = df['publish_date'].to_numpy().astype('datetime64[M]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b490c454",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS =  set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef3b1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add words that aren't in the NLTK stopwords list\n",
    "new_stopwords = ['says','say','australia','australian','2015','2010','2011','2013','2012','2014','2017','2018','2019','2020',\n",
    "                'january','february','march','april','may','june','july','august','september','october','november','decemeber']\n",
    "new_stopwords_list = STOPWORDS.union(new_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9a20d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##REMOVE STOPWORDS\n",
    "\n",
    "df['headline_text'] = df['headline_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (new_stopwords_list)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bafac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Add year column\n",
    "df['year']    = pd.DatetimeIndex(df['publish_date']).year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7748e03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##uncomment for individual PM\n",
    "dfgrouped = df.groupby(['publish_date']).count().reset_index()\n",
    "dfgrouped= dfgrouped[dfgrouped['publish_date'] > '2010-01-01']\n",
    "dfgrouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26649adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df= df[df['publish_date'] > '2010-01-01']\n",
    "\n",
    "yearly = df.groupby('year')['headline_text'].agg(list).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8a6e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "##loop through year images and create simple word clouds\n",
    "\n",
    "yearlist = [2015,2016,2017,2018,2019,2020]\n",
    "\n",
    "for i in yearlist: \n",
    "    mask = np.array(Image.open('/Users/jackohagan/datajournalism/abcheadlines/{}.jpg'.format(i)))\n",
    "    wc = WordCloud(mask=mask, background_color=\"rgba(255, 255, 255, 0)\",\n",
    "                   max_words=150, max_font_size=256,\n",
    "                   random_state=42, width=mask.shape[1],\n",
    "                   height=mask.shape[0])\n",
    "\n",
    "    # use ImageColorGenerator to generate the colors from the image\n",
    "    image_colors = ImageColorGenerator(mask)\n",
    "    \n",
    "    plt.subplot(1, 2, 1)   # Define 3 rows, 2 column, Activate subplot 1. \n",
    "\n",
    "    wc.generate(' '.join(yearly.get(i)))\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.savefig(\"{}-cloudraw.png\".format(i),dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ab6f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "##simple word clouds not that interesting - create some analytical ones\n",
    "from tqdm import tqdm #used for progress bar\n",
    "import math\n",
    "\n",
    "#list of years\n",
    "year = df.year.unique()\n",
    "\n",
    "###create a list of headlines for each year using a list comprehension\n",
    "corpus = [' '.join(df[(df.year==years)].headline_text.tolist()) for years in year]\n",
    "\n",
    "\n",
    "# import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#instantiate a CountVectorizer object\n",
    "# utilize the new STOP_WORDS list\n",
    "cv=CountVectorizer( stop_words=new_stopwords_list, ngram_range=(1, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41575681",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fit transform our text and create a dataframe with the result\n",
    "X = cv.fit_transform(corpus)\n",
    "X = X.toarray()\n",
    "\n",
    "bow=pd.DataFrame(X, columns = cv.get_feature_names())\n",
    "bow.index=year\n",
    "\n",
    "##filter to have at least 25 mentions\n",
    "bow = bow.loc[:, (bow.sum() >= 25)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1122b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "##credit to https://towardsdatascience.com/generate-meaningful-word-clouds-in-python-5b85f5668eeb\n",
    "\n",
    "from tqdm import tqdm #used for progress bar\n",
    "import math\n",
    "\n",
    "#build an empty dataframe\n",
    "t_bow_df=pd.DataFrame()\n",
    "\n",
    "# By years, divide the number of times a word is said, \n",
    "# by the total number of times they've said all the words that they have said (frequency)\n",
    "bow_transformed = bow.apply(lambda x: (x+1)/ (bow.loc[x.name].sum()+1), axis=1) \n",
    "\n",
    "for years in tqdm(year):\n",
    "    feat_s = bow[bow.index != years].sum()+1 #sum all the times each word was said excluding current years\n",
    "    feat_s_all = np.sum(bow[bow.index != years].sum()) # all the total words that were said, excluding the current years\n",
    "    tot = feat_s / feat_s_all #find the frequency of words said among other years\n",
    "    row = bow_transformed.loc[years] / tot #divide the years array of frequency of words by all other years' frequency\n",
    "    row = pd.DataFrame(row).T #create a df, and transform\n",
    "    results = row.apply(lambda x: math.log(x,2)) #take the log of each frequency\n",
    "    \n",
    "    t_bow_df = pd.concat([t_bow_df, pd.DataFrame([results], index=[years])]) #append to df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1ea374",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "t_bow_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac43ef7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "yearlist = [2015,2016, 2017,2018, 2019, 2020]\n",
    "\n",
    "\n",
    "for i in yearlist: \n",
    "    mask = np.array(Image.open('/Users/jackohagan/datajournalism/abcheadlines/{}.jpg'.format(i)))\n",
    "    wc = WordCloud(mask=mask, background_color=\"rgba(255, 255, 255, 0)\",\n",
    "                   max_words=200, max_font_size=256,\n",
    "                   random_state=42, width=mask.shape[1],\n",
    "                   height=mask.shape[0], )\n",
    "\n",
    "    # use ImageColorGenerator to generate the colors from the image\n",
    "    image_colors = ImageColorGenerator(mask)\n",
    "\n",
    "    text2_dict=t_bow_df.loc[i].sort_values(ascending=False).to_dict()\n",
    "\n",
    "    wc.generate_from_frequencies(text2_dict)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.savefig(\"{}-cloud_25.png\".format(i),dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62735dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "##top words per year\n",
    "yearly = df.groupby(['year'], as_index = False).agg({'headline_text': ' '.join})\n",
    "\n",
    "yearly_n1_word =[]\n",
    "yearly_n2_word =[]\n",
    "yearly_n3_word =[]\n",
    "yearly_n4_word =[]\n",
    "yearly_n5_word =[]\n",
    "\n",
    "\n",
    "\n",
    "yearly_word_freqs = []\n",
    "for year in tqdm(yearly.year):\n",
    "    F = nltk.FreqDist([i for i in ' '.join(yearly.query(f'year=={year}').headline_text).split() if i not in new_stopwords_list])\n",
    "    yearly_word_freqs.append(F)\n",
    "    result = [i for i in F.most_common(10)]\n",
    "    yearly_n1_word.append(result[0])\n",
    "    yearly_n2_word.append(result[1])\n",
    "    yearly_n3_word.append(result[2])\n",
    "    yearly_n4_word.append(result[3])\n",
    "    yearly_n5_word.append(result[4])\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc281596",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=df, x='publish_date', y='headline_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7b27fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#what are the top words each year?\n",
    "\n",
    "yearly_n1_word =[]\n",
    "yearly_n2_word =[]\n",
    "yearly_n3_word =[]\n",
    "\n",
    "yearly_word_freqs = []\n",
    "for year in tqdm(yearly.year):\n",
    "    F = nltk.FreqDist([i for i in ' '.join(yearly.query(f'year=={year}').text).split() if i not in STOPWORDS])\n",
    "    yearly_word_freqs.append(F)\n",
    "    result = [i for i in F.most_common(3)]\n",
    "    yearly_n1_word.append(result[0])\n",
    "    yearly_n2_word.append(result[1])\n",
    "    yearly_n3_word.append(result[2])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97f7287",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['publish_date'] = df['publish_date'] + Week(weekday=0)\n",
    "df['publish_date'] \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dfb024",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df[df['publish_date'] > '2007-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdabdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##sentiment of different politicans - word cloud?\n",
    "##word cloud for different politicians\n",
    "\n",
    "##topics that come up\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccc97f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##ADD Booleans for selecting headlines\n",
    "df['Scott Morrison'] = df['headline_text'].str.contains(\"scott morrison\")\n",
    "df['Melbourne'] = df['headline_text'].str.contains(\"melbourne\")\n",
    "df['Sydney'] = df['headline_text'].str.contains(\"sydney\")\n",
    "df['Adelaide'] = df['headline_text'].str.contains(\"adelaide\")\n",
    "df['Brisbane'] = df['headline_text'].str.contains(\"brisbane\")\n",
    "df['Hobart'] = df['headline_text'].str.contains(\"hobart\")\n",
    "df['Perth'] = df['headline_text'].str.contains(\"perth\")\n",
    "df['Canberra'] = df['headline_text'].str.contains(\"canberra\")\n",
    "df['scomo'] = df['headline_text'].str.contains(\"scomo\")\n",
    "df['Malcolm Turnbull'] = df['headline_text'].str.contains(\"malcolm turnbull\")\n",
    "\n",
    "df['albanese'] = df['headline_text'].str.contains(\"albanese\")\n",
    "\n",
    "df['Kevin Rudd'] = df['headline_text'].str.contains(\"kevin rudd\")\n",
    "\n",
    "df['Julia Gillard'] = df['headline_text'].str.contains(\"julia gillard\")\n",
    "\n",
    "df['Tony Abbott'] = df['headline_text'].str.contains(\"tony abbott\")\n",
    "\n",
    "df['murdoch'] = df['headline_text'].str.contains(\"murdoch\")\n",
    "\n",
    "df['robodebt'] = df['headline_text'].str.contains(\"robodebt\")\n",
    "\n",
    "df['robodebt'] = df['headline_text'].str.contains(\"robodebt\")\n",
    "\n",
    "\n",
    "df['sausage'] = df['headline_text'].str.contains(\"sausage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d095ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2d9594b",
   "metadata": {},
   "source": [
    "Dataset appears to have been cleaned up slightly, possibly removing stop words. we find on ABC website the article name might be:\n",
    "\n",
    "What changes to national security is Tony Abbott proposing?\n",
    "\n",
    "but our dataset has \n",
    "\n",
    "\n",
    "what changes to national security is tony abbott"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dd1687",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['sausage'] == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1d995b",
   "metadata": {},
   "source": [
    "Visualise trends over time, by date - animated graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8de4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc09f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfgrouped = df.groupby(['publish_date']).sum().cumsum()\n",
    "dfgrouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8ba8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfgroupedselected = dfgrouped[['Kevin Rudd','Julia Gillard','Tony Abbott','Malcolm Turnbull','Scott Morrison']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6ab2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##plot animated for politicians\n",
    "dfgroupedselected.plot_animated(\"abc-count-politicians.gif\",    figsize=(6.5*1.5, 3.5*1.5),\n",
    "                     title=\"Cumulative ABC Headlines\",\n",
    "                     kind='line',\n",
    "                                period_label=False,\n",
    "                               steps_per_period=1, interpolate_period=False, period_length=50,\n",
    "                                 label_events={\n",
    "        'Morrison PM':datetime.strptime(\"24/08/2018\", \"%d/%m/%Y\"),\n",
    "        'Turnbull PM':datetime.strptime(\"15/09/2015\", \"%d/%m/%Y\"),\n",
    "        'Abbott PM':datetime.strptime(\"18/09/2013\", \"%d/%m/%Y\"),\n",
    "        'Rudd 2nd term':datetime.strptime(\"27/06/2013\", \"%d/%m/%Y\"),\n",
    "        'Gillard PM':datetime.strptime(\"24/06/2010\", \"%d/%m/%Y\"),\n",
    "        'Rudd PM':datetime.strptime(\"03/12/2007\", \"%d/%m/%Y\")},\n",
    "    dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9866e764",
   "metadata": {},
   "outputs": [],
   "source": [
    "##group for cities\n",
    "dfgrouped = df.groupby(['publish_date']).sum().cumsum()\n",
    "\n",
    "##cities\n",
    "dfgroupedselected = dfgrouped[['Sydney','Melbourne','Adelaide','Hobart', 'Canberra']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7226aca",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289324b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##plot animated for cities\n",
    "dfgroupedselected.plot_animated(\"abc-count-cities.gif\",    figsize=(6.5*1.5, 3.5*1.5),\n",
    "                     title=\"Cumulative ABC Headlines\",\n",
    "                     kind='line',\n",
    "                                period_label=False, enable_progress_bar = True,\n",
    "                               steps_per_period=1, interpolate_period=False, period_length=50,\n",
    "    dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8b4c66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
